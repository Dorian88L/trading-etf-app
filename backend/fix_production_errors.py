#!/usr/bin/env python3
"""
Script de correction rapide pour les erreurs de production
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from sqlalchemy import text
from app.core.database import engine, SessionLocal
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def fix_database_issues():
    """Corrige les probl√®mes de base de donn√©es."""
    db = SessionLocal()
    
    try:
        logger.info("üîß Correction des probl√®mes de base de donn√©es...")
        
        # 1. Nettoyer les donn√©es en double dans market_data
        logger.info("üßπ Nettoyage des doublons dans market_data...")
        
        # Supprimer les doublons en gardant le plus r√©cent
        cleanup_sql = """
        DELETE FROM market_data 
        WHERE ctid NOT IN (
            SELECT MAX(ctid) 
            FROM market_data 
            GROUP BY time, etf_isin
        );
        """
        
        result = db.execute(text(cleanup_sql))
        deleted_rows = result.rowcount
        logger.info(f"‚úÖ {deleted_rows} doublons supprim√©s")
        
        # 2. Mettre √† jour les donn√©es NULL qui causent les erreurs de calcul
        logger.info("üîß Correction des valeurs NULL...")
        
        # Corriger les change_absolute et change_percent NULL
        update_sql = """
        UPDATE market_data 
        SET 
            change_absolute = 0.0, 
            change_percent = 0.0 
        WHERE change_absolute IS NULL OR change_percent IS NULL;
        """
        
        db.execute(text(update_sql))
        logger.info("‚úÖ Valeurs NULL corrig√©es")
        
        # 3. Nettoyer les market_data trop anciens (garde 7 jours)
        cleanup_old_sql = """
        DELETE FROM market_data 
        WHERE time < NOW() - INTERVAL '7 days';
        """
        
        result = db.execute(text(cleanup_old_sql))
        deleted_old = result.rowcount
        logger.info(f"‚úÖ {deleted_old} anciennes donn√©es supprim√©es")
        
        # 4. Reindexer pour optimiser les performances
        logger.info("üìä R√©indexation...")
        
        db.execute(text("REINDEX INDEX market_data_pkey;"))
        db.execute(text("ANALYZE market_data;"))
        
        db.commit()
        logger.info("‚úÖ Base de donn√©es nettoy√©e et optimis√©e")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la correction de la base: {e}")
        db.rollback()
        return False
        
    finally:
        db.close()

def check_database_health():
    """V√©rifie la sant√© de la base de donn√©es."""
    db = SessionLocal()
    
    try:
        logger.info("üîç V√©rification de la sant√© de la base...")
        
        # V√©rifier les doublons
        duplicate_check = """
        SELECT COUNT(*) as duplicates
        FROM (
            SELECT time, etf_isin, COUNT(*) 
            FROM market_data 
            GROUP BY time, etf_isin 
            HAVING COUNT(*) > 1
        ) as dups;
        """
        
        result = db.execute(text(duplicate_check)).fetchone()
        duplicates = result[0]
        
        if duplicates > 0:
            logger.warning(f"‚ö†Ô∏è {duplicates} doublons d√©tect√©s")
        else:
            logger.info("‚úÖ Aucun doublon trouv√©")
        
        # V√©rifier les valeurs NULL probl√©matiques
        null_check = """
        SELECT COUNT(*) as nulls
        FROM market_data 
        WHERE change_absolute IS NULL OR change_percent IS NULL;
        """
        
        result = db.execute(text(null_check)).fetchone()
        nulls = result[0]
        
        if nulls > 0:
            logger.warning(f"‚ö†Ô∏è {nulls} valeurs NULL trouv√©es")
        else:
            logger.info("‚úÖ Aucune valeur NULL probl√©matique")
        
        # Statistiques g√©n√©rales
        stats_sql = """
        SELECT 
            COUNT(*) as total_records,
            COUNT(DISTINCT etf_isin) as unique_etfs,
            MAX(time) as latest_data
        FROM market_data;
        """
        
        result = db.execute(text(stats_sql)).fetchone()
        total, unique_etfs, latest = result
        
        logger.info(f"üìä Statistiques: {total} enregistrements, {unique_etfs} ETFs uniques")
        logger.info(f"üìÖ Derni√®res donn√©es: {latest}")
        
        return duplicates == 0 and nulls == 0
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la v√©rification: {e}")
        return False
        
    finally:
        db.close()

def main():
    """Point d'entr√©e principal."""
    logger.info("üöÄ D√©marrage des corrections de production...")
    
    # V√©rifier l'√©tat initial
    logger.info("üîç V√©rification de l'√©tat initial...")
    initial_health = check_database_health()
    
    if not initial_health:
        logger.info("üîß Correction des probl√®mes d√©tect√©s...")
        if fix_database_issues():
            logger.info("‚úÖ Corrections appliqu√©es avec succ√®s")
        else:
            logger.error("‚ùå √âchec des corrections")
            return 1
    
    # V√©rifier l'√©tat final
    logger.info("üîç V√©rification finale...")
    final_health = check_database_health()
    
    if final_health:
        logger.info("üéâ Base de donn√©es en bon √©tat!")
        logger.info("")
        logger.info("üìã Actions recommand√©es:")
        logger.info("1. Red√©marrer le backend pour appliquer les correctifs du code")
        logger.info("2. Surveiller les logs pour v√©rifier l'absence d'erreurs")
        logger.info("3. Tester les endpoints qui √©chouaient")
        logger.info("")
        logger.info("üîÑ Pour red√©marrer le backend:")
        logger.info("   docker restart backend-prod")
        return 0
    else:
        logger.error("‚ùå Probl√®mes persistants d√©tect√©s")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)