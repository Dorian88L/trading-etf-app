"""
Endpoints optimis√©s pour les donn√©es ETF avec sources multiples
"""

from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import asyncio
import redis
import json
import logging

from app.core.database import get_db
from app.api.deps import get_current_active_user
from app.models.user import User
from app.services.multi_source_etf_data import get_multi_source_etf_service, MultiSourceETFDataService, ETFDataPoint, DataSource
from app.services.dynamic_etf_service import get_dynamic_etf_service, DynamicETFService
from app.core.config import settings

logger = logging.getLogger(__name__)
router = APIRouter()

# Cache Redis pour les donn√©es ETF
try:
    redis_client = redis.Redis.from_url(settings.REDIS_URL, decode_responses=True)
except:
    redis_client = None
    logger.warning("Redis non disponible, cache d√©sactiv√©")

@router.get(
    "/optimized-etfs",
    tags=["optimized-market"],
    summary="ETFs europ√©ens optimis√©s multi-sources",
    description="""
    R√©cup√®re les donn√©es ETF europ√©ens depuis sources multiples avec fallback intelligent.
    
    **Sources utilis√©es dans l'ordre de priorit√© :**
    1. üü¢ **Yahoo Finance** : Source principale, donn√©es gratuites et fiables
    2. üü° **Alpha Vantage** : Fallback 1, 25 requ√™tes/jour gratuit
    3. üü° **Financial Modeling Prep** : Fallback 2, 250 requ√™tes/jour gratuit
    4. üü° **EODHD** : Fallback 3, 20 requ√™tes/jour gratuit
    5. üîµ **Donn√©es hybrides** : Dernier recours, calcul√©es selon tendances march√©
    
    **Optimisations :**
    - Cache intelligent Redis (5 minutes)
    - Rate limiting automatique par source
    - Retry automatique avec sources alternatives
    - Score de confiance pour chaque donn√©e
    
    **ETFs Europ√©ens support√©s :**
    - üá¨üáß London Stock Exchange (.L) : IWDA, VWRL, CSPX, VUSA, etc.
    - üá©üá™ XETRA (.DE) : EUNL, XMME, EXS1, SX5E
    - üá´üá∑ Euronext Paris (.PA) : CW8, CAC
    - üá≥üá± Euronext Amsterdam (.AS) : IWDA, VWRL
    
    **Donn√©es retourn√©es :**
    - Prix actuels et variations R√âELLES
    - Volume de trading
    - Secteur et bourse de cotation
    - ISIN et devise
    - Source utilis√©e et score de confiance
    """,
    response_description="Liste des ETFs avec donn√©es temps r√©el optimis√©es"
)
async def get_optimized_etf_data(
    use_cache: bool = True,
    min_confidence: float = 0.0,
    dynamic_service: DynamicETFService = Depends(get_dynamic_etf_service)
):
    """
    R√©cup√®re les donn√©es ETF optimis√©es avec sources multiples
    
    Args:
        use_cache: Utiliser le cache Redis (d√©faut: True)
        min_confidence: Score de confiance minimum (0.0 √† 1.0)
        etf_service: Service de donn√©es ETF multi-sources
        
    Returns:
        Dict contenant la liste des ETFs avec m√©tadonn√©es
    """
    try:
        cache_key = f"optimized_etfs_v2:{min_confidence}"
        
        # V√©rifier le cache Redis
        if use_cache and redis_client:
            try:
                cached_data = redis_client.get(cache_key)
                if cached_data:
                    logger.info("Donn√©es ETF r√©cup√©r√©es depuis le cache Redis")
                    return json.loads(cached_data)
            except Exception as e:
                logger.warning(f"Erreur cache Redis: {e}")
        
        # R√©cup√©rer les donn√©es depuis les sources dynamiques
        logger.info("R√©cup√©ration des donn√©es ETF depuis configuration dynamique...")
        etf_data_list = await dynamic_service.get_all_realtime_data_for_etf_list()
        
        # Filtrer selon le score de confiance
        filtered_etfs = [
            etf for etf in etf_data_list 
            if etf.confidence_score >= min_confidence
        ]
        
        # Convertir en format API
        etf_api_data = []
        source_stats = {}
        
        for etf in filtered_etfs:
            # Statistiques par source
            source_name = etf.source.value
            if source_name not in source_stats:
                source_stats[source_name] = 0
            source_stats[source_name] += 1
            
            etf_item = {
                'symbol': etf.symbol,
                'isin': etf.isin,
                'name': etf.name,
                'current_price': etf.current_price,
                'change': etf.change,
                'change_percent': etf.change_percent,
                'volume': etf.volume,
                'market_cap': etf.market_cap,
                'currency': etf.currency,
                'exchange': etf.exchange,
                'sector': etf.sector,
                'last_update': etf.last_update.isoformat(),
                'source': etf.source.value,
                'confidence_score': etf.confidence_score,
                
                # M√©tadonn√©es utiles pour le frontend
                'is_real_data': etf.source != DataSource.HYBRID,
                'data_quality': 'excellent' if etf.confidence_score >= 0.9 else 'good' if etf.confidence_score >= 0.7 else 'fair',
                'reliability_icon': 'üü¢' if etf.confidence_score >= 0.9 else 'üü°' if etf.confidence_score >= 0.7 else 'üü†'
            }
            etf_api_data.append(etf_item)
        
        # Trier par score de confiance et nom
        etf_api_data.sort(key=lambda x: (-x['confidence_score'], x['name']))
        
        response_data = {
            'status': 'success',
            'count': len(etf_api_data),
            'data': etf_api_data,
            'timestamp': datetime.now().isoformat(),
            'metadata': {
                'sources_used': source_stats,
                'avg_confidence': sum(etf.confidence_score for etf in filtered_etfs) / len(filtered_etfs) if filtered_etfs else 0,
                'real_data_percentage': round((len([etf for etf in filtered_etfs if etf.source != DataSource.HYBRID]) / len(filtered_etfs)) * 100, 1) if filtered_etfs else 0,
                'cache_used': False,
                'next_update_in': 300  # 5 minutes
            }
        }
        
        # Mettre en cache pour 5 minutes
        if redis_client:
            try:
                redis_client.setex(cache_key, 300, json.dumps(response_data, default=str))
                logger.info("Donn√©es ETF mises en cache Redis")
            except Exception as e:
                logger.warning(f"Erreur mise en cache Redis: {e}")
        
        logger.info(f"Donn√©es ETF r√©cup√©r√©es: {len(etf_api_data)} ETFs depuis {len(source_stats)} sources")
        return response_data
        
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration ETF optimis√©e: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es ETF: {str(e)}")

@router.get(
    "/optimized-etf/{symbol}",
    tags=["optimized-market"],
    summary="Donn√©es ETF sp√©cifique optimis√©es",
    description="""
    R√©cup√®re les donn√©es d'un ETF sp√©cifique avec sources multiples.
    
    **Param√®tres :**
    - `symbol` : Symbole ETF (ex: IWDA.L, VWRL.L, CSPX.L)
    
    **Retourne :**
    - Donn√©es temps r√©el de l'ETF
    - Source utilis√©e et score de confiance
    - M√©tadonn√©es de qualit√©
    """,
    response_description="Donn√©es d√©taill√©es de l'ETF"
)
async def get_optimized_single_etf_data(
    symbol: str,
    use_cache: bool = True,
    etf_service: MultiSourceETFDataService = Depends(get_multi_source_etf_service)
):
    """
    R√©cup√®re les donn√©es d'un ETF sp√©cifique
    
    Args:
        symbol: Symbole ETF (ex: IWDA.L)
        use_cache: Utiliser le cache Redis
        etf_service: Service de donn√©es ETF
        
    Returns:
        Dict contenant les donn√©es de l'ETF
    """
    try:
        cache_key = f"optimized_etf:{symbol}"
        
        # V√©rifier le cache
        if use_cache and redis_client:
            try:
                cached_data = redis_client.get(cache_key)
                if cached_data:
                    return json.loads(cached_data)
            except Exception as e:
                logger.warning(f"Erreur cache Redis pour {symbol}: {e}")
        
        # R√©cup√©rer les donn√©es
        etf_data = await etf_service.get_etf_data(symbol)
        
        if not etf_data:
            raise HTTPException(status_code=404, detail=f"ETF {symbol} non trouv√©")
        
        response_data = {
            'status': 'success',
            'symbol': etf_data.symbol,
            'data': {
                'symbol': etf_data.symbol,
                'isin': etf_data.isin,
                'name': etf_data.name,
                'current_price': etf_data.current_price,
                'change': etf_data.change,
                'change_percent': etf_data.change_percent,
                'volume': etf_data.volume,
                'market_cap': etf_data.market_cap,
                'currency': etf_data.currency,
                'exchange': etf_data.exchange,
                'sector': etf_data.sector,
                'last_update': etf_data.last_update.isoformat(),
                'source': etf_data.source.value,
                'confidence_score': etf_data.confidence_score,
                'is_real_data': etf_data.source != DataSource.HYBRID,
                'data_quality': 'excellent' if etf_data.confidence_score >= 0.9 else 'good' if etf_data.confidence_score >= 0.7 else 'fair'
            },
            'timestamp': datetime.now().isoformat()
        }
        
        # Cache pour 2 minutes
        if redis_client:
            try:
                redis_client.setex(cache_key, 120, json.dumps(response_data, default=str))
            except Exception as e:
                logger.warning(f"Erreur mise en cache pour {symbol}: {e}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration ETF {symbol}: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration de {symbol}: {str(e)}")

@router.get(
    "/data-sources-status",
    tags=["optimized-market"],
    summary="Statut des sources de donn√©es",
    description="""
    Retourne le statut des diff√©rentes sources de donn√©es ETF.
    
    **Informations fournies :**
    - Rate limits par source
    - Disponibilit√© des API keys
    - Statistiques d'utilisation
    - Recommandations
    """,
    response_description="Statut d√©taill√© des sources de donn√©es"
)
def get_data_sources_status():
    """
    Retourne le statut des sources de donn√©es
    """
    import os
    from datetime import datetime
    
    # V√©rification des cl√©s API depuis les variables d'environnement
    alpha_vantage_key = os.getenv('ALPHA_VANTAGE_API_KEY')
    fmp_key = os.getenv('FINANCIAL_MODELING_PREP_API_KEY') 
    eodhd_key = os.getenv('EODHD_API_KEY')
    twelvedata_key = os.getenv('TWELVEDATA_API_KEY')
    finnhub_key = os.getenv('FINNHUB_API_KEY')
    
    current_time = datetime.now().isoformat()
    sources_status = {}
    
    # Yahoo Finance (toujours disponible)
    sources_status['yahoo_finance'] = {
        'api_key_available': True,
        'rate_limit': 'unlimited',
        'calls_used': 'N/A',
        'calls_remaining': 'unlimited',
        'window_seconds': 86400,
        'status': 'available',
        'last_reset': current_time,
        'notes': 'Source principale gratuite'
    }
    
    # Alpha Vantage
    if alpha_vantage_key:
        sources_status['alpha_vantage'] = {
            'api_key_available': True,
            'rate_limit': 25,
            'calls_used': 0,
            'calls_remaining': 25,
            'window_seconds': 86400,
            'status': 'available',
            'last_reset': current_time,
            'notes': 'API gratuite 25 req/jour'
        }
    
    # Financial Modeling Prep
    if fmp_key:
        sources_status['financial_modeling_prep'] = {
            'api_key_available': True,
            'rate_limit': 250,
            'calls_used': 0,
            'calls_remaining': 250,
            'window_seconds': 86400,
            'status': 'available',
            'last_reset': current_time,
            'notes': 'API gratuite 250 req/jour'
        }
    
    # Statistiques globales
    available_sources = len([s for s in sources_status.values() if s['status'] == 'available'])
    total_sources = len(sources_status)
    
    return {
        'status': 'success',
        'sources': sources_status,
        'summary': {
            'available_sources': available_sources,
            'total_sources': total_sources,
            'reliability_score': available_sources / max(total_sources, 1),
            'primary_source': 'yahoo_finance',
            'fallback_sources': available_sources - 1
        },
        'recommendations': [
            "Yahoo Finance est la source principale",
            f"{available_sources} source(s) configur√©e(s)",
            "Ajoutez plus de cl√©s API pour une meilleure fiabilit√©"
        ],
        'timestamp': current_time
    }

@router.post(
    "/refresh-cache",
    tags=["optimized-market"],
    summary="Rafra√Æchir le cache des donn√©es ETF",
    description="Force le rafra√Æchissement du cache Redis pour les donn√©es ETF",
    response_description="Confirmation du rafra√Æchissement"
)
async def refresh_etf_cache(
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_active_user),
    etf_service: MultiSourceETFDataService = Depends(get_multi_source_etf_service)
):
    """
    Force le rafra√Æchissement du cache des donn√©es ETF
    """
    try:
        if redis_client:
            # Supprimer les cl√©s de cache
            pattern = "optimized_etfs_v2:*"
            keys = redis_client.keys(pattern)
            if keys:
                redis_client.delete(*keys)
                logger.info(f"Cache ETF vid√©: {len(keys)} cl√©s supprim√©es")
        
        # Lancer le rafra√Æchissement en arri√®re-plan
        background_tasks.add_task(etf_service.get_all_etf_data)
        
        return {
            'status': 'success',
            'message': 'Cache rafra√Æchi, nouvelles donn√©es en cours de r√©cup√©ration',
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Erreur rafra√Æchissement cache: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur lors du rafra√Æchissement: {str(e)}")